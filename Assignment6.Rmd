---
title: "SVM Regression"
author: "Meinhard Capucao, Khang Thai"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: inline
---
### Introduction
For this project, I used the diamond.csv data set from previous projects to determine the svm linear regression. We used the depth, table, x (length in mm), y (width in mm), and z (height in mm) to determine the carat value. 

## Read in the data
The dataset is around 53k dataset and we decided to lower the dataset to 10k so that the amount of time it takes to run svm is a lot less.
```{r}
library(e1071)
DiamondDataset <- read.csv("diamonds.csv", header=TRUE)
DiamondDataset <- DiamondDataset[1:10000,c(2,6,7,8,9,10,11)]
str(DiamondDataset)
```
## Spliting the data into Test, Train, and Validate 
```{r}
library(caret)
set.seed(1234)
spec <- c(train=.6, test=.2, validate=.2)
i <- sample(cut(1:nrow(DiamondDataset), nrow(DiamondDataset)*cumsum(c(0,spec)), labels = names(spec)))

train <- DiamondDataset[i == "train",]
test <- DiamondDataset[i == "test",]
vald <- DiamondDataset[i =="validate",]
```

```{r}
library(tidyverse)
lm1 <- lm(carat ~., data = train)
summary(lm1)
par(mfrow=c(2,2))
plot(lm1)
```
Based on the graphs, the data has some outliers but there is a general pattern with the data when trying to predict the karat. 

```{r}
pred <- predict(lm1, newdata = test)
cor_lm <- cor(pred, test$carat)
mse_lm <- mean((pred - test$carat)^2)
print(paste("Cor = ", cor_lm))
print(paste("MSE = ", mse_lm))
```
Here we can see that the correlation between the carat and all the predictors is around 98.74% which is really good. This indicates that 
## SVM Linear
```{r}
svm1 <- svm(carat~., data=train, kernel = "linear", cost = 10, scale=TRUE)
summary(svm1)
```

```{r}
pred <- predict(svm1, newdata = test)
cor_svm1 <- cor(pred, test$carat)
mse_svm1 <- mean((pred = test$carat)^2)
print(paste("Cor_svm1 = ", cor_svm1))
print(paste("MSE_svm1 = ", mse_svm1))
```
Using Linear SVM we were able to achieve a similar results. Here we have a 98.75% correlation using SVM with a cost of 10. 

## Tune
```{r}
tune_svm1 <- tune(svm, carat~., data=vald, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)
```


## Evaluate Best Linear SVM
```{r}
pred <- predict(tune_svm1$best.model, newdata=test)
cor_svm1_tune <- cor(pred, test$carat)
mse_svm1_tune <- mean((pred = test$carat)^2)
print(paste("Cor_svm1_tune = ", cor_svm1_tune))
print(paste("MSE_svm1_tune = ", mse_svm1_tune))
```
Here we used the best model which was using a cost of .001 in order to find the best correlation between the predictors and the karat. However, it seems that using a cost of .001 did not give us the best correlation as it falls less than the correlation of a cost of 10. Our error and dispersion is also really low so there will be little to no difference between the costs. 

## Polynomial Kernel
```{r}
svm2 <- svm(carat~., data=train, kernel="polynomial", cost=10, scale=TRUE)
summary(svm2)
```

```{r}
pred <- predict(svm2, newdata=test)
cor_svm2 <- cor(pred, test$carat)
mse_svm2 <- mean((pred = test$carat)^2)
print(paste("Cor_svm2 = ", cor_svm2))
print(paste("MSE_svm2 = ", mse_svm2))
```
Here we can see that the correlation of the Polynomial kernel shows that there is only a 44% correlation between the predictors and the karat. We didn't bother testing different cost because there will probably not be a big difference from 44%. Lets move on to Radial kernel and see if it is better than linear.
## Radial Kernel
```{r}
svm3 <- svm(carat~., data=train, kernel="radial", cost=10,gamma=1, scale=TRUE)
summary(svm3)
```

```{r}
pred <- predict(svm3, newdata=test)
cor_svm3 <- cor(pred, test$carat)
mse_svm3 <- mean((pred = test$carat)^2)
print(paste("Cor_svm3 = ", cor_svm3))
print(paste("MSE_svm3 = ", mse_svm3))
```
Amazing! As we can see, using a radial kernel with a cost of 10 resulted in a 99% correlation between the predictors and the karat. This is the best kernel to use to predict the karat based on the predictors.
## Tune Hyperperameters

```{r}
set.seed(1234)
tune.out <- tune(svm, carat~., data = vald, kernel="radial", ranges = list(cost=c(0.1,1,10,100,1000), gamma=c(0.5,1,2,3,4)))
summary(tune.out)
```

Because we know that the radial kernel is the best kernel, we can start trying to tune the gamma to try to find the best gamma. 

```{r}
svm4 <- svm(carat~., data = train, kernel = "radial", cost = 100, gamma = 1, scale=TRUE)
summary(svm4)
```

```{r}
pred <- predict(svm4, newdata=test)
cor_svm4 <- cor(pred,test$carat)
mse_svm4 <- mean((pred - test$carat)^2)
print(paste("Cor_svm4 = ", cor_svm4))
print(paste("MSE_svm4 = ", mse_svm4))
```
Here we tested the results if we used a gamma of 1 and got a 99% correlation which is pretty good but we can get a better correlation. 
```{r}
svm4 <- svm(carat~., data = train, kernel = "radial", cost = 100, gamma = 0.5, scale=TRUE)
summary(svm4)
```


```{r}
pred <- predict(svm4, newdata=test)
cor_svm4 <- cor(pred,test$carat)
mse_svm4 <- mean((pred - test$carat)^2)
print(paste("Cor_svm4 = ", cor_svm4))
print(paste("MSE_svm4 = ", mse_svm4))
```
Here we can see that having a higher Cost and lower gamma will results in a higher correlation. We were able to get a 99.38% correlation with a gamma of 0.5 and a cost of 100. This is the best result for finding the the karat based on the predictors.
